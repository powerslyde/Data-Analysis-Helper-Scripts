{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the external packages into Python to perform various analyses\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk import punkt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import json\n",
    "import pymssql as pym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import custom functions\n",
    "from cleanString import cleanString\n",
    "from createStopWords import createStopWords\n",
    "from fileDataLoad import fileDataLoad\n",
    "from WarehouseDataExtract import WarehouseDataExtract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using my test star wars dataset - if this was a real analysis you would use another source file, \n",
    "# either from Excel, CSV or the AMS Warehouse using WarehouseData Extract\n",
    "# filename = r'C:\\Users\\jpuryear1\\Documents\\Python Scripts\\starwars_data.xlsx'\n",
    "# sheetname = 'Sheet1'\n",
    "# inputDF = fileDataLoad(filename,sheetname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please type in the EAI code that you would like to generate a wordcloud for:  10966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are pulling data for 10966\n"
     ]
    }
   ],
   "source": [
    "# Imports the JSON file with User Id and Password\n",
    "db_data = json.load(open(r'c:\\Users/jpuryear1/Documents/Python Scripts/DB_connection.json'))\n",
    "# Extracts user id and password to variables\n",
    "WH_USER = db_data['userid']\n",
    "WH_PW = db_data['password']\n",
    "#EAI code to be queried\n",
    "#print('Welcome to Incident Wordcloud Generation!')\n",
    "eai_cd = input('Please type in the EAI code that you would like to generate a Top Terms List for: ')\n",
    "print(f'You are pulling data for {eai_cd}')\n",
    "#print('Data Extract Commencing - Please note that it will take several minutes to pull the data')\n",
    "#Standard SQL to pull data for a given EAI Code\n",
    "sql = f\"\"\"SELECT number, short_description, description, u_resolution_notes, u_incident_resolution_category, \n",
    "u_incident_resolution_subcateg, close_code, contact_type\n",
    "  FROM [USBPMMetricsWhse].[dbo].[T_SRVNW_INCDN_SUM] a\n",
    "  JOIN [USBPMMetricsWhse].[dbo].[T_SRVNW_AFCT_TASK_CI_SUM] b on a.number = b.task\n",
    "  WHERE\n",
    "  a.DW_REC_CUR_IND = 'Y'\n",
    "  and b.ci_item like '{eai_cd}%'\n",
    "  and a.opened_at > '2019-01-01 00:00:00'\"\"\"\n",
    "# runs the WarehouseDataExtract function and imports the data to a dataframe\n",
    "inputDF = WarehouseDataExtract(WH_USER, WH_PW, sql)\n",
    "# print the head so you know which column to pull the text data from\n",
    "inputDF['Summary'] = inputDF['short_description'].astype(str) + ' - ' + inputDF['description'].astype(str) + ' - ' + inputDF['u_resolution_notes'].astype(str)\n",
    "print(f\"There are {inputDF.shape[0]} observations and {inputDF.shape[1]} features in this dataset. \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Summary data from the dataframe into a list\n",
    "TextReviewList = inputDF.loc[:,'Summary'].tolist()\n",
    "#Create a \"clean\" list to hold the cleaned strings\n",
    "TextCleanList = []\n",
    "# Clean the strings from TextReviewList and copy the clean strings to TextCleanList\n",
    "for str in TextReviewList:\n",
    "    TextCleanList.append(cleanString(str))  \n",
    "# adds a new column to the inputDF to hold the cleaned summary text\n",
    "inputDF['CleanText'] = ''\n",
    "# merge the cleaned summary back into the input dataframe\n",
    "inputDF['CleanText'] = pd.Series(TextCleanList).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stopword list: use the \"remove_words\" list to add stop words to the list\n",
    "remove_words = ('entered', 'auto', 'ams', 'arm', 'metlife', 'l1', 'us', 'corporate', 'billing', 'system', 'byauto','aalert', 'f090', 'e90', 'gssp', 'platformentered'\n",
    "               ,'com', 'https', 'gto', 'bmc')\n",
    "stopset = createStopWords(remove_words)\n",
    "#need more information about what this does...\n",
    "vectorizer = TfidfVectorizer(stop_words=stopset, analyzer = 'word')\n",
    "#need more information about what this does...\n",
    "tfidf_matrix = vectorizer.fit_transform(inputDF.loc[:,'CleanText'].tolist())\n",
    "# break out the distinct words\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "dense = tfidf_matrix.todense()\n",
    "denselist = dense.tolist()\n",
    "df = pd.DataFrame(denselist, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the Df dataframe to True / False float values\n",
    "df_boo = (df.loc[:] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum each column\n",
    "terms = df_boo.iloc[:, 1:].sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define each column as a pandas series\n",
    "termSeries = pd.Series(terms.index.values, index = np.arange(len(terms)))\n",
    "count = pd.Series(list(terms), index = np.arange(len(terms)))\n",
    "# create the dataframe\n",
    "term_df = pd.DataFrame(dict(termSeries = termSeries, count = count))\n",
    "term_df = term_df[[\"termSeries\", \"count\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   termSeries  count\n",
      "0       error    355\n",
      "1      online    323\n",
      "2    platform    323\n",
      "3         msg    318\n",
      "4      public    317\n",
      "5       topaz    317\n",
      "6        step    314\n",
      "7       10966    313\n",
      "8    critical    312\n",
      "9   keepalive    308\n",
      "10  servicing    306\n",
      "11       html    306\n",
      "12    content    303\n",
      "13  servi001a    302\n",
      "14      match    302\n",
      "15     impact    245\n",
      "16     closed    237\n",
      "17    manager    237\n",
      "18     events    235\n",
      "19   original    235\n",
      "20     please    153\n",
      "21      taken    114\n",
      "22    actions    112\n",
      "23    provide    111\n",
      "24   incident    110\n"
     ]
    }
   ],
   "source": [
    "#sort in descending order\n",
    "term_df.sort_values([\"count\"], ascending=False, inplace=True)\n",
    "term_df.reset_index(inplace=True, drop=True)\n",
    "# Print out the Top 25 Terms used at least once per Row\n",
    "print(term_df.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#term_df.head(25).plot(kind='barh')\n",
    "#plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
